{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fe7b190",
   "metadata": {},
   "source": [
    "# Week 3 - Classical ML Models - Part II\n",
    "\n",
    "## 1. Support Vector Machine\n",
    "\n",
    "During the previous week, we learned how to use logistic regression for binary classification problems.\n",
    "\n",
    "This week, we are going to look at another type of supervised model that can be used for classification problems - **Support vector machine** (or **SVM**).\n",
    "\n",
    "### Introduction\n",
    "\n",
    "![SVM](https://miro.medium.com/max/625/1*ala8WX2z47WYpn932hUkhA.jpeg)\n",
    "\n",
    "SVM is a supervised ML algorithm that is most commonly used for binary classification problems. As in the previous cases, the training of such model involves passing set of examples $(x_i, y_i)$. For instance, we want to build a model for spam detection. In such case, the feature ($x_i$) could contain the word / link count, while the label ($y_i$) could be either 1 (spam) or 0 (not-spam).\n",
    "\n",
    "So far, we have a general understanding about the training process of SVM, on the other hand, we have not yet covered the hypothesis function we are trying to fit throughout training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8feb13",
   "metadata": {},
   "source": [
    "### Hypothesis\n",
    "\n",
    "The goal of SVM algorithm is to find a hyperplane (boundary line) with the following properties:\n",
    "- It creates separation with the maximum margin between variable classes\n",
    "- Equation $>1$ output for positive class and $<-1$ for examples in negative class\n",
    "\n",
    "In mathematical terms this can be written as:\n",
    "- $\\hat{y} = -1$ if $w^T*x + b < 0$\n",
    "- $\\hat{y} = 1$ if $w^T*x + b \\geqslant 0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68c8cea",
   "metadata": {},
   "source": [
    "### Cost function\n",
    "\n",
    "As in the previous models, SVM has a cost function associated to it. As we have two main goals: minimizing the individual distances ($w$) between the data point and hyperline, and maximizing the margin width.\n",
    "\n",
    "Therefore, our cost function has two parts:\n",
    "\n",
    "$J(w) = \\frac{[w]^2}{2} + C[\\frac{1}{N}\\sum_{i}^n max(0, 1 - y_i(wx_1 + b))]$\n",
    "\n",
    "The second half of the function is also called hinge loss. In addition, we are using a regularization constant $C$ as a way of weighting misclassification.\n",
    "\n",
    "In python code, the cost function can be expressed in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af9adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(W, X, Y):\n",
    "    # calculate hinge loss\n",
    "    N = X.shape[0]\n",
    "    distances = 1 - Y * (np.dot(X, W))\n",
    "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "    hinge_loss = reg_strength * (np.sum(distances) / N)\n",
    "    \n",
    "    # calculate cost\n",
    "    cost = 1 / 2 * np.dot(W, W) + hinge_loss\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1627721",
   "metadata": {},
   "source": [
    "As we now have our cost function, we need to find a way to optimize it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae885d7",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Similar to the logistic regression model, we are going to apply the gradient descent algorithm for finding the minimize our cost function. After taking the partial derivative in respect to $w$, we get the following system:\n",
    "- $\\frac{1}{N}\\sum_{i}^n w$ if $max(0, 1 - y_i*(wx_i)) = 0$\n",
    "- $\\frac{1}{N}\\sum_{i}^n w - Cy_ix_i$ otherwise\n",
    "\n",
    "Such system has the following Python implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2df77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_gradient(W, X_batch, Y_batch):\n",
    "    # if only one example is passed (eg. in case of SGD)\n",
    "    if type(Y_batch) == np.float64:\n",
    "        Y_batch = np.array([Y_batch])\n",
    "        X_batch = np.array([X_batch])\n",
    "    distance = 1 - (Y_batch * np.dot(X_batch, W))\n",
    "    dw = np.zeros(len(W))\n",
    "    for ind, d in enumerate(distance):\n",
    "        if max(0, d) == 0:\n",
    "            di = W\n",
    "        else:\n",
    "            di = W - (reg_strength * Y_batch[ind] * X_batch[ind])\n",
    "        dw += di\n",
    "    dw = dw/len(Y_batch)  # average\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65795f08",
   "metadata": {},
   "source": [
    "After finding the gradient of the cost function, we have to update our weights which can be done in a quite similar manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7dc556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(features, outputs):\n",
    "    max_epochs = 5000\n",
    "    weights = np.zeros(features.shape[1])\n",
    "    # stochastic gradient descent\n",
    "    for epoch in range(1, max_epochs): \n",
    "        # shuffle to prevent repeating update cycles\n",
    "        X, Y = shuffle(features, outputs)\n",
    "        for ind, x in enumerate(X):\n",
    "            ascent = calculate_cost_gradient(weights, x, Y[ind])\n",
    "            weights = weights - (learning_rate * ascent)\n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6bdad8",
   "metadata": {},
   "source": [
    "### Sklearn implementation\n",
    "\n",
    "As in the previous cases, instead of building our model from scratch, we can save some time and use sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27950b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf = svm.SVC(kernel = 'linear')\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eadf26",
   "metadata": {},
   "source": [
    "As you may notice here, we have defined **kernel** parameter. As you may remember from the start, some parameter correlations might not be linear: the hyperline might have to take circle for polynomial forms to differentiate variable classes. The kernel parameter defines the form of the this hyperline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b6785f",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Now, it's time to apply our skills. For this purpose, we are going to use the breast cancer patients data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "301bec8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8392c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.load_breast_cancer()\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "#Split data\n",
    "X_train, X_test, y_train, y_test =\n",
    "\n",
    "#Selecting SVM model with 'linear' kernel\n",
    "\n",
    "\n",
    "#Fitting model into train dataset\n",
    "\n",
    "\n",
    "#Save predictions to y_pred variable\n",
    "y_pred = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce9f27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd2437b",
   "metadata": {},
   "source": [
    "### Logistic regression vs SVM\n",
    "\n",
    "We can see that SVM and logistic regression has many similarities: both use lines to differentiate classes to solve classification problems. As a result, both models (in most cases) can be used as substitutes for one another without a larger drop in accuracy.\n",
    "\n",
    "On the other hand, it is useful to know cases when one of the models provides a better computational performance:\n",
    "- When the number of features is large (1 - 10000) and number of training examples (10 - 10000) is small, use **logistic regression** or **SVM with a linear kernel**.\n",
    "- When the number of features is small (1 - 1000) and number of training examples is intermediate (10 - 10000), use **SVM**.\n",
    "- When number of features is small (1 - 1000) and number of training examples is large (50000 - 1000000), use **logistic regression** or **SVM with a linear kernel**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
