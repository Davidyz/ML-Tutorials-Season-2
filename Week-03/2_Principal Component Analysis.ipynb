{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24e21ff7",
   "metadata": {},
   "source": [
    "# Week 3 - Classical ML Models - Part II\n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "Principal components analysis (or PCA) is a technique used for dimensionality reduction enabling to identify signifficant correlations in the dataset. Consequently, it allows to reduce the number of dimensions within the dataset without lossing important information.\n",
    "\n",
    "### Why do we need PCA?\n",
    "\n",
    "As we have seen so far, ML models has a tendency to work better with larger datasets: the good amount of data allows training of more accurate model. On the other hand, as we increase the dataset dimensions, we observe a few negative effects:\n",
    "- In large dimensional datasets, there are a lot of inconsistencies reducing model's accuracy\n",
    "- The usage of redundant features increases the computational time.\n",
    "\n",
    "This is where the dimensional reduction comes in - it helps to extract only the most signifficant correlations and reduce the number of dimensions.\n",
    "\n",
    "### PCA computation steps\n",
    "\n",
    "#### 1. Standardization\n",
    "\n",
    "In short, the data standardization involves taking values and scalling them into a similar range. It ensures less biased model training process as larger values no longer shift whole model.\n",
    "\n",
    "It is carried out by subtracting each value by the mean and dividing by deviation.\n",
    "\n",
    "![standardization](https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2019/08/Standardization-Principal-Component-Analysis-Edureka-300x77.png)\n",
    "\n",
    "#### 2. Computing covariance matrix\n",
    "\n",
    "As it has been mentioned earlier, PCA helps to find the correlations within the dataset. These correlations between different dataset variables can be expressed in a covariance matrix.\n",
    "\n",
    "Mathematically speaking, covariance matrix can be imagined as a  p x p matrix, where p represent the dimensions.\n",
    "\n",
    "For example, let's say we have a 2-dimensional dataset consisting of variables a and b. In such case, the covariance matrix can be expressed as:\n",
    "![covariance matrix](https://d1jnx9ba8s6j9r.cloudfront.net/blog/wp-content/uploads/2019/08/Covariance-Matrix-Principal-Component-Analysis-Edureka-150x61.png)\n",
    "\n",
    "- $Cov(a, a)$ shows the covariance between the variable and itself\n",
    "- $Cov(a, b)$ shows the covariance between two variables\n",
    "\n",
    "#### 3. Eigenvalues and eigenvectors\n",
    "\n",
    "Let's say we have a 3-dimensional matrix **A** containing dataset variables:\n",
    "![matrix](https://miro.medium.com/max/139/1*OBDgTXEUlUt3wfKblp47BQ.png)\n",
    "\n",
    "According to theory, if we multiply the matrix by a vector and the resulting vector differs from the original vector by a scalar value, such vector is called **eigenvector**. The scalar value in such expression becomes **eigenvalue**. In mathematical terms - $A.x = lambda.x$ or  $A.x - lambda.x = 0$.\n",
    "\n",
    "In such condition, the determinant of the characteristic function has to be equal to 0 or in other words:\n",
    "\n",
    "$det|A - lambda.I|$, where $I$ is identity function.\n",
    "\n",
    "To better understand this, let's analyze an example with 2-dimensions:\n",
    "\n",
    "![matrix](https://miro.medium.com/max/130/1*4htJZnnvPc6CLad3IqPLbw.png)\n",
    "\n",
    "After multilplying $\\lambda$ by identity matrix and subtracting from the 2-dimensional matrix, we get:\n",
    "![matrix](https://miro.medium.com/max/144/1*eZSqgvsRvB8-sahbKqseGQ.png)\n",
    "\n",
    "After determining the determinant and solving it, we get the following eigenvalues: $\\frac{5}{2} + i\\frac{\\sqrt{15}}{2}$ and $\\frac{5}{2} - i\\frac{\\sqrt{15}}{2}$.\n",
    "\n",
    "To get the eigenvectors, we have to simply substitute the eigenvalues into the equation: $det|A - \\lambda.I|$\n",
    "\n",
    "#### 4. Computing principal components\n",
    "\n",
    "In short, principal components are new set of variables obtained from the initial dataset that are signifficant an independent of each other. Once we computed the eigenvectors and eigenvalues, we have to order them in the descending order (first component is formed from eigenvector with the heighest eigenvalue and so on).\n",
    "\n",
    "\n",
    "#### 5. Reducing the dimensions of the dataset\n",
    "\n",
    "The last step is to re-arrange the original data accoring to variables' signifficance. In order to do so, we simply multiply the transpose of the original data by the transpose of the feature vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44c052",
   "metadata": {},
   "source": [
    "### Python implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e15e50",
   "metadata": {},
   "source": [
    "Such pipeline would take the following code form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fca7999",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca(X):\n",
    "    #Scaling values\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    #Computing covariance matrix\n",
    "    mean = np.mean(X, axis = 0)\n",
    "    cov_mat = (X - mean).T.dot((X - mean)) / (X.shape[0]-1)\n",
    "    \n",
    "    #Calculating eigenvectors and eigenvalues\n",
    "    cov_mat = np.cov(X.T)\n",
    "    eig_vals, eig_vecs = np.linalg.eig(cov_mat)\n",
    "    \n",
    "    #Computing feature vectors\n",
    "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "    \n",
    "    return eig_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870bafa3",
   "metadata": {},
   "source": [
    "However, as in the previous examples, we Scikit-learn library provides functions for performing PCA computations which reduces the coding time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29c41b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA using scikit-learn\n",
    "def pca(X):\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit_transform(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
