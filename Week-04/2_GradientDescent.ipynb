{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8ce78ad",
   "metadata": {},
   "source": [
    "# Gradient Descent \n",
    "quick video: https://www.youtube.com/watch?v=ySDX02WD0og\n",
    "\n",
    "Gradient descent is a technique used to find a minimum using numerical analysis in times where directly computing the minimum analytically is too hard or even infeasible. The idea behind the algorithm is simple. At any given point in time, you determine the effect of modifying your input variable on the cost function you are trying to minimize. In the one dimensional case, if increasing the value of your input decreases your cost function, then you can increase the value of your input by a small step to get you closer to the minimum cost. Generalizing to higher dimensions, the gradient tells you the direction of the greatest increase on your cost function, so you can move the input in the opposite direction to hopefully decrease the value of the cost function.\n",
    "\n",
    "<img src=\"https://shashank-ojha.github.io/ParallelGradientDescent/non-convex.png\" width =\"500\" height=500 >\n",
    "\n",
    "\n",
    "There are multiple variations to the gradient descent algorithm such as Batch Gradient Descent, Stochastic Gradient Descent, Mini-Batch Gradient Descent, etc. But for the focus at this stage, we're only covering the fundamentals of gradient descent before moving on to more complex versions. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3957fc",
   "metadata": {},
   "source": [
    "## 1.1 Derivation of Equation\n",
    "Now let's derive the derivative of our MSE loss with respect to $\\theta_0$ and $\\theta_1$. Recall that MSE is expressed as:\n",
    "\n",
    "$$mse(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} ((\\theta_0 + \\theta_1 x_i) - y_i)^2$$\n",
    "\n",
    "Let's now compute the partial derivate of the loss with regard to parameter $\\theta_0$, our bias term:\n",
    "\n",
    "$$\\frac{\\partial mse(\\theta)}{\\partial \\theta_0} = \\frac{2}{m}\\sum_{i=1}^{m}  ((\\theta_0 + \\theta_1 x_i) - y_i) \\cdot 1 $$\n",
    "\n",
    "And the partial derivate of the loss with regard to parameter $\\theta_1$:\n",
    "\n",
    "$$\\frac{\\partial mse(\\theta)}{\\partial \\theta_1} = \\frac{2}{m} \\sum_{i=1}^{m} ((\\theta_0 + \\theta_1 x_i) - y_i) \\cdot x_i$$\n",
    "\n",
    "\n",
    "Here is our update rule for the parameters $\\theta_0$ and $\\theta_1$ at iteration $t$, using gradient descent:\n",
    "\n",
    "$$\\theta_i^{(t+1)} = \\theta_i^t - \\eta \\cdot \\frac{\\partial mse(\\theta)}{\\partial \\theta_i} $$\n",
    "\n",
    "Algorithm: \n",
    "\n",
    "REPEAT UNTIL CONVERGENCE{\n",
    "    $$\\theta_i^{(t+1)} = \\theta_i^t - \\eta \\cdot \\frac{2}{m} \\sum_{i=1}^{m} ((\\theta_i + \\theta_1 x_i) - y_i) \\cdot x_i$$\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d806fc9",
   "metadata": {},
   "source": [
    "### 1.2 Generalization of Equation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e2238e",
   "metadata": {},
   "source": [
    "$$a_{n+1} = a_{n} - \\gamma \\nabla f(a_n) $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab00cee",
   "metadata": {},
   "source": [
    "$$f(a_1) \\geq f(a_2) \\geq f(a_3) \\geq ... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f604e989",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/1280px-Gradient_descent.svg.png\" width =\"300\" height=300 >\n",
    "\n",
    "Hopefully this monotonic sequence $(a_n)$ converges to a local minimum. Provided some assumptions about our function $f(a_n)$,  such as the function being convex and $\\nabla f(a_n)$ being lipschitz. Since a local minima of a convex function is also a global minima, we can guarantee with our assumptions that the sequence will converge. \n",
    "\n",
    "For non-convex functions (usually the case of deep neural networks). There has been a myriad of research on how to get gradient descent to converge to a global minimum, but in practice the problem with gradient descent being stuck on a saddle or a local minimum actually rarely pops up. \n",
    "\n",
    "Take a look at this paper that talks about saddle points and how we can guarantee gradient descent to converge to negative infinity almost surely: https://arxiv.org/abs/1602.04915\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0bf99b",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent \n",
    "\n",
    "The downside of gradient descent is that we have to compute the sum of all the gradients before we update the weights. Stochastic gradient descent (SGD) tries to lower the computation per iteration, at the cost of an increased number of iterations necessary for convergence.\n",
    "\n",
    "Instead of computing the sum of all gradients, stochastic gradient descent selects an observation uniformly at random. While this is a noisy estimator, we are able to update the weights much more frequency and therefore hope to converge more rapidly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3866e62",
   "metadata": {},
   "source": [
    "### Algorithm:\n",
    "   \n",
    "   ### Loop\n",
    "   \n",
    "   ### for i=1 to m \n",
    "$$ \\theta_i^{(t+1)} = \\theta_i^t - \\eta \\cdot((\\theta_0 + \\theta_1 x_i) - y_i)^2 $$ \n",
    "       \n",
    "       \n",
    "   \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847a7214",
   "metadata": {},
   "source": [
    "In this algorithm, we repeatedly run through the training set, and each time we encounter a training example, we update the parameters according to the gradient of the error with respect to that single training example only. This algorithm is called stochastic gradient descent (also incremental gradient descent). Whereas batch gradient descent has to scan through the entire training set before taking a single step—a costly operation if m is large—stochastic gradient descent can start making progress right away, and continues to make progress with each example it looks at. Often, stochastic gradient descent gets θ “close” to the minimum much faster than batch gra- dient descent. (Note however that it may never “converge” to the minimum, and the parameters θ will keep oscillating around the minimum of J(θ); but in practice most of the values near the minimum will be reasonably good approximations to the true minimum.2) For these reasons, particularly when the training set is large, stochastic gradient descent is often preferred over batch gradient descent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b3c046",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/908/1*FXHp55rpDM0tkaD5oz3Dvg.png\" width =\"500\" height=500 >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79119c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
