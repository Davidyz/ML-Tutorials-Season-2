{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caabbc2a",
   "metadata": {},
   "source": [
    "# Challenge - Credit Card Fraud Detection\n",
    "\n",
    "![neurons](https://www.gannett-cdn.com/media/USATODAY/USATODAY/2013/02/05/afp-517027843-16_9.jpg?width=3200&height=1680&fit=crop)\n",
    "\n",
    "## Background information\n",
    "\n",
    "Many credit bank companies and banks are aiming at developing sophisticated algorithms to recognize fraudulent credit card transactions. For this task, we will look at a reduced dataset containing transaction time, amount and not-described variables (due to confidentiality issues).\n",
    "\n",
    "Your task is to build a working neural network (**from scratch**) that would predict fraudulent transactions.\n",
    "\n",
    "\n",
    "## Data\n",
    "\n",
    "For this week's challenge, we again are going to use the Kaggle InClass competition link of which you can find [here](https://www.kaggle.com/c/ucl-ai-society-card-fraud-detection/data).\n",
    "\n",
    "As in the previous InClass competition, you will need to train your model on the ```train.csv``` dataset, while the submission file should be produced from the ```test.csv```.\n",
    "\n",
    "As you might notice, some of the bits (**including the model**) are already written for you to reduce time. Therefore, just by following this notebook, you should be able to produce a decent submission file. Although, feel free to change the activation functions, number of layers and neurons, data splitting  or standardization functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d747200",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from math import log2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f557ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read train data\n",
    "PATH = \n",
    "data = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068599d0",
   "metadata": {},
   "source": [
    "### EDA\n",
    "\n",
    "Similar to the previous challenges, we are first going to start with exploratory data analysis. As usual, we will have a look at the head values of our data, check whether there are any Null values and do some initial visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef51d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Have a look at top dataset values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d91a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bc4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check if there are any null values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7abdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the number of values for each category (1 - fraudulent, 0 - non-fraudulent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c14822",
   "metadata": {},
   "source": [
    "As it can be seen from the cell above, the number of fraudulent and non-fraudulent data cases is not equal (the original dataset had even larger difference). Keep this in mind when choosing splitting function.\n",
    "\n",
    "Now, let's plot some initial graphs to observe any correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c37076",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this cell\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.pairplot(data[[\"Time\", \"V2\", \"Amount\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000e9758",
   "metadata": {},
   "source": [
    "As we can seen, some of the graphs (*Time-V2*, *Time-Amount*) have a linear relationship except for a few outliers. As the fraudulent cases will most probably be outliers, let's look at one of these graphs more in depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7995f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this cell\n",
    "plt.figure(figsize=(20,5))\n",
    "sns.scatterplot(data[\"Time\"], data[\"V2\"], hue = data[\"Class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ab8713",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Before building our model, we first need to do some data preprocessing. First, we will need to extract the features and labels. In addition, we are going to drop some of the less relevant features that could distort the model training (you can play around with the categories)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368117c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting features-labels and dropping some of the columns\n",
    "X = \n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbf199c",
   "metadata": {},
   "source": [
    "As it was discussed at the start of the lecture, the data scaling can be very beneficial especially when dealing with outlier and large number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3581ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaling features (use RobustScaler)\n",
    "X = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66dca9c9",
   "metadata": {},
   "source": [
    "Finally, convert your features and labels to numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52acd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert labels to an array and expand dimensions in axis = -1\n",
    "y = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc74a450",
   "metadata": {},
   "source": [
    "### Dimensional reduction\n",
    "\n",
    "As we saw in the EDA section, our data contains 28 features that are not very informative. Logically, we will need to reduce the size of our feature dataset which can be achieved with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f5495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pass features through PCA\n",
    "pca = PCA(2)\n",
    "X = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9151c6b9",
   "metadata": {},
   "source": [
    "### Data splitting\n",
    "\n",
    "Finally, split the dataset into train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24132bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68840164",
   "metadata": {},
   "source": [
    "Before building our model, we first need to transpose our train and test arrays (to exchange the dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5750df6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transpose splitted arrays\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b53f39b",
   "metadata": {},
   "source": [
    "## Creating model\n",
    "\n",
    "Finally, we can build our neural network. To make everything easier to use, the whole model is built as a class object. Since the problem itself is quite complex, we are going to use multiple dense layers and nodes. For the activation functions, the hidden layers will have the *relu* function, while for the last layer, we are going to use *sigmoid* function.\n",
    "\n",
    "As we are dealing with the categorical inbalanced dataset, we are going to use *cross entropy loss*.\n",
    "\n",
    "**You will only need to fill commented fields**. In other words, your code should go in every ___."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701ee78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_dimension, units, activation='', multiplier=0.01):\n",
    "        \n",
    "        self.weights, self.bias = self.initialize(input_dimension, units, multiplier)\n",
    "        \n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = activation\n",
    "            self.activation_forward = self.sigmoid\n",
    "            self.activation_backward = self.sigmoid_grad\n",
    "        elif activation == 'relu':\n",
    "            self.activation = activation\n",
    "            self.activation_forward = self.relu\n",
    "            self.activation_backward = self.relu_grad\n",
    "\n",
    "    def initialize(self, input_size, nodes, multiplier):\n",
    "        #Initialize weights and biases\n",
    "        weights = multiplier * np.random.randn(___, ___)\n",
    "        bias = np.zeros([___, 1])\n",
    "        return weights, bias\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        \n",
    "        #Define sigmoid function\n",
    "        A = ___\n",
    "        return A\n",
    "        \n",
    "    def sigmoid_grad(self, dA):\n",
    "\n",
    "        s = 1 / (1 + np.exp(-self.prevZ))\n",
    "        dZ = dA * s * (1 - s)\n",
    "        return dZ\n",
    "    \n",
    "    \n",
    "    def relu(self, Z):\n",
    "        \n",
    "        #Define relu function\n",
    "        \n",
    "        A = ___\n",
    "        return A\n",
    "        \n",
    "    def relu_grad(self, dA):\n",
    "\n",
    "        s = np.maximum(0, self.prevZ)\n",
    "        dZ = (s>0) * 1 * dA\n",
    "        return dZ \n",
    "        \n",
    "    \n",
    "    def forward(self, A):\n",
    "\n",
    "        Z = np.dot(self.weights, A) + self.bias\n",
    "        self.prevZ = Z\n",
    "        self.prevA = A\n",
    "        A = self.activation_forward(Z)\n",
    "        return A\n",
    "    \n",
    "    \n",
    "    def backward(self, dA):\n",
    "\n",
    "        dZ = self.activation_backward(dA)\n",
    "        m = self.prevA.shape[1]\n",
    "        self.dW = 1 / m * np.dot(dZ, self.prevA.T)\n",
    "        self.db = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "        prevdA = np.dot(self.weights.T, dZ)\n",
    "        return prevdA\n",
    "    \n",
    "    \n",
    "    def update(self, lr):\n",
    "\n",
    "        self.weights = self.weights - lr * self.dW\n",
    "        \n",
    "        #Similarly to the weights update, define next bias value\n",
    "        self.___ = self.___ - lr * self.___\n",
    "\n",
    "        \n",
    "    def output_dimension(self):\n",
    "\n",
    "        return len(self.bias)\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \n",
    "    def __init__(self, multiplier = 0.01):\n",
    "\n",
    "        self.layers=[]\n",
    "        self.multiplier = multiplier\n",
    "        self.loss_function = self.cross_entropy_loss\n",
    "        self.loss_backward = self.cross_entropy_loss_grad\n",
    "        \n",
    "    def add_layer(self, input_dimension=None, units=1, activation=''):\n",
    "        \n",
    "        if (input_dimension is None):\n",
    "            input_dimension=self.layers[-1].output_dimension()\n",
    "        layer = DenseLayer(input_dimension, units, activation, multiplier= self.multiplier)\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def cross_entropy_loss(self, Y, A, epsilon=1e-15):\n",
    "\n",
    "        m = Y.shape[1]\n",
    "        loss = -1 * (Y * np.log(A + epsilon) + (1 - Y) * np.log(1 - A + epsilon))\n",
    "        cost = 1 / m * np.sum(loss)\n",
    "        return np.squeeze(cost)\n",
    "            \n",
    "    def cross_entropy_loss_grad(self, Y, A):\n",
    "\n",
    "        dA = -(np.divide(Y, A) - np.divide(1 - Y, 1 - A))\n",
    "        return dA\n",
    "\n",
    "    \n",
    "    def cost(self, Y, A):\n",
    "\n",
    "        return self.loss_function(Y, A)\n",
    "\n",
    "        \n",
    "    def forward(self, X):\n",
    "\n",
    "        x = np.copy(X)\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "                \n",
    "    def backward(self, A, Y):\n",
    "\n",
    "        dA = self.loss_backward(Y, A)\n",
    "        for layer in reversed(self.layers):\n",
    "            dA = layer.backward(dA)\n",
    "    \n",
    "    \n",
    "    def update(self, lr=0.03):\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.update(lr)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3290ef8",
   "metadata": {},
   "source": [
    "As we have defined our model, now it's time to actually use it. Try different layer and node combinations for the maximum accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce871482",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting input dimension\n",
    "input_size = ___\n",
    "\n",
    "#Creating model object\n",
    "model = NeuralNetwork()\n",
    "\n",
    "#Adding layers\n",
    "model.add_layer(input_dimension=___, units=___, activation=___)\n",
    "\n",
    "#Adding final layer\n",
    "model.add_layer(units=_, activation=___)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7741b8",
   "metadata": {},
   "source": [
    "After creating and optimizing the model, test it by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd4ca03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#State the number of iterations\n",
    "num_iterations = ___\n",
    "\n",
    "def round_value(A):\n",
    "    return np.uint8(A > 0.5)\n",
    "\n",
    "def accuracy(yhat, Y):\n",
    "    return round(np.sum(yhat==Y) / len(yhat.flatten()) * 1000) / 10\n",
    "\n",
    "for idx in range(1, num_iterations+1):\n",
    "    A = model.forward(X_train)\n",
    "    model.backward(A, y_train)\n",
    "    model.update(lr=0.03)\n",
    "    if idx % 5 == 0:\n",
    "        yhat = round_value(A)\n",
    "        print('cost:', model.cost(y_train, A), f'\\taccuracy: {accuracy(yhat, y_train)}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
