{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APhzWS-BTqRU"
   },
   "source": [
    "# Convolutional Neural Networks from Scratch\n",
    "\n",
    "Here is the complete code for the CNN from scratch notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4NzaVImUkSm"
   },
   "source": [
    "## Convolutions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLtEGLqRUOnU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Conv:\n",
    "  def __init__(self, in_channels, out_channels, filter_size, pad = 0, stride = 1, weight_scale = 1e-3):\n",
    "    self.pad = pad\n",
    "    self.stride = stride\n",
    "    # Weights initialized from 0-centered Gaussian with standard deviation\n",
    "    # equal to `weight_scale` (by default set to 1e-3) for stability\n",
    "    self.filters =  np.random.randn(out_channels, in_channels, \n",
    "                                     filter_size, filter_size) * weight_scale\n",
    "    # biases initialized to zero\n",
    "    self.bias = np.zeros((out_channels, ))\n",
    "\n",
    "\n",
    "  def calc_output_dim(self, x):\n",
    "    _, H, W = x.shape\n",
    "    _, _, filter_height, filter_width = self.filters.shape\n",
    "\n",
    "    H_out = 1 + (H + 2 * self.pad - filter_height) // self.stride\n",
    "    W_out = 1 + (W + 2 * self.pad - filter_width) // self.stride\n",
    "\n",
    "    return H_out, W_out\n",
    "\n",
    "\n",
    "  def iterate_regions(self, x):\n",
    "    H_out, W_out = self.calc_output_dim(x)\n",
    "    F, C, filter_height, filter_width = self.filters.shape\n",
    "\n",
    "    for i in range(H_out):\n",
    "      for j in range(W_out):\n",
    "        x_region = x[:, i * self.stride : i * self.stride + filter_height, j * self.stride : j * self.stride + filter_width]\n",
    "        yield i, j, x_region\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: An ndarray containing input data, of shape (C, H, W)\n",
    "    Returns:\n",
    "    - out: Result of convolution, of shape (F, H', W') \n",
    "    where F is the number of out channels and \n",
    "    H' = 1 + (H + 2 * pad - filter_height) / stride\n",
    "    W' = 1 + (W + 2 * pad - filter_width) / stride\n",
    "    \"\"\"\n",
    "\n",
    "    self.x = x  # save as cache\n",
    "\n",
    "    _, H, W = x.shape\n",
    "    F, _, _, _ = self.filters.shape\n",
    "\n",
    "    # to pad last two dimensions of the four dimensional tensor\n",
    "    x_pad = np.pad(x, self.pad, mode='constant')\n",
    "\n",
    "    H_out, W_out = self.calc_output_dim(x)\n",
    "    out = np.zeros((F, H_out, W_out))\n",
    "\n",
    "    for f in range(F):\n",
    "      # convolve F times to get F x filter_height x filter_width \n",
    "      kernel = self.filters[f] # dimension is C x filter_height x filter_width    \n",
    "      for i, j, x_region in self.iterate_regions(x_pad):\n",
    "        out[f, i, j] = (x_region * kernel).sum()\n",
    "      \n",
    "      out[f] += self.bias[f] # makes use of broadcasting\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "  def backward(self, dout):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - dout: An ndarray of the upstream derivatives, of shape (F, H', W')\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient w.r.t input x, of shape (C, H, W)\n",
    "    - dw: Gradient w.r.t. filters, of shape (F, C, filter_height, filter_width)\n",
    "    - db: Gradient w.r.t the biases, of shape (F, )\n",
    "    \"\"\"\n",
    "\n",
    "    _, H, W = self.x.shape # retrieve from cache\n",
    "    F, C, HH, WW = self.filters.shape\n",
    "    \n",
    "    x_pad = np.pad(self.x, self.pad, mode='constant')\n",
    "\n",
    "    dx = np.zeros_like(x_pad) \n",
    "    dw = np.zeros_like(self.filters) \n",
    "    db = np.zeros_like(self.bias)\n",
    "\n",
    "    for f in range(F): \n",
    "      for i, j, x_region in self.iterate_regions(x_pad):\n",
    "        dw[f] += dout[f, i, j] * x_region\n",
    "\n",
    "        dx[:, i*self.stride:i*self.stride + HH, j*self.stride:j*self.stride + WW] += \\\n",
    "              dout[f, i, j] * self.filters[f]\n",
    "\n",
    "      db[f] += dout[f].sum()\n",
    "\n",
    "    # trim off padding so that dimension of dx is the same as x\n",
    "    dx = dx[:, self.pad: self.pad + H, self.pad : self.pad + W]\n",
    "\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bWtkocrb8tF"
   },
   "source": [
    "#### (Aside) Quick sanity check\n",
    "\n",
    "The cells below serve as a quick sanity check to make sure that you aren't getting any errors in your implementation and that the methods are returning ndarrays of the correct shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LP1GWCbCb9_R"
   },
   "outputs": [],
   "source": [
    "dout = np.random.randn(8, 26, 26)\n",
    "dx, dw, db = conv.backward(dout)\n",
    "\n",
    "print(\"dx shape: \", dx.shape)\n",
    "print(\"dw shape: \", dw.shape)\n",
    "print(\"db shape: \", db.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-mWoXsHDcANy"
   },
   "outputs": [],
   "source": [
    "# create dummy image\n",
    "img = np.random.randn(3, 28, 28) # assume 28 x 28 RGB image\n",
    "\n",
    "conv = Conv(in_channels=3, out_channels=8, filter_size=3) \n",
    "out = conv.forward(img)\n",
    "\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6YCcVkiWYhk"
   },
   "source": [
    "## Pooling layers : max pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfQlv4CAM6qO"
   },
   "outputs": [],
   "source": [
    "class MaxPool2x2:\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: An ndarray containing input data, of shape (C, H, W)\n",
    "    Returns:\n",
    "    - out: Result of max pooling operation, of shape (C, H', W') where\n",
    "    H' = 1 + (H - pooling height) // stride\n",
    "    W' = 1 + (H - pooling width) // stride\n",
    "    \"\"\"\n",
    "    self.x = x # cache\n",
    "    C, H, W = x.shape\n",
    "    out = np.zeros((C, H // 2, W // 2))\n",
    "\n",
    "    for c in range(C):\n",
    "      for i in range(H // 2):\n",
    "        for j in range(W // 2):\n",
    "          out[c, i, j] = np.max(x[c, i*2:i*2 + 2, j*2:j*2 + 2])\n",
    "\n",
    "    return out\n",
    "\n",
    "  def backward(self, dout):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - dout: Upstream gradients, of shape (C, H', W')\n",
    "    Returns:\n",
    "    - dx: Gradient w.r.t input x\n",
    "    \"\"\"\n",
    "    C, H, W = self.x.shape\n",
    "\n",
    "    dx = np.zeros_like(self.x) # C x H x W\n",
    "\n",
    "    for c in range(C):\n",
    "      for i in range(H // 2):\n",
    "        for j in range(W // 2):\n",
    "          window = self.x[c, i*2:i*2 + 2, j*2:j*2 + 2]\n",
    "          max_idx = np.argmax(window)\n",
    "          # window.shape is (2, 2)\n",
    "          max_r = max_idx // window.shape[1]\n",
    "          max_c = max_idx % window.shape[1]\n",
    "          dx[c, i*2:i*2+2, j*2:j*2+2][max_r, max_c] = dout[c, i, j]\n",
    "\n",
    "    return dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwSHWO7aXtT3"
   },
   "source": [
    "#### (Aside) See it in action\n",
    "\n",
    "Below is the code that demonstrates how `forward()` and `backward()` work for the max pooling layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zT0otxySV3r"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "x_train = x_train[:, np.newaxis, :,:]\n",
    "x_test = x_test[:, np.newaxis,:,:]\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "img = x_train[0]\n",
    "label = y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1636925738999,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "4T1UldnUSoIV",
    "outputId": "dbbed0e3-657d-4e45-bc7b-5173d6c1292c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.3647059 , 0.9882353 , 0.99215686, 0.73333335],\n",
       "        [0.        , 0.9764706 , 0.99215686, 0.9764706 ],\n",
       "        [0.7176471 , 0.99215686, 0.99215686, 0.8117647 ],\n",
       "        [0.99215686, 0.99215686, 0.98039216, 0.7137255 ]]], dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img[:,16:20,16:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1636925739622,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "oCw6Gu5rSlKc",
    "outputId": "5ebefa43-d0b4-448c-d6e9-263c754930e0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.98823529, 0.99215686],\n",
       "        [0.99215686, 0.99215686]]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pool = MaxPool2x2()\n",
    "out = pool.forward(img[:,16:20,16:20])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1636925740621,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "2V7exaxDXEVR",
    "outputId": "2c9d69f2-1298-4776-ccdd-49088e3520f0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.        , 0.9882353 , 0.99215686, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ],\n",
       "        [0.        , 0.99215686, 0.99215686, 0.        ],\n",
       "        [0.        , 0.        , 0.        , 0.        ]]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx = pool.backward(out)\n",
    "dx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_o6mV_WdcHJ"
   },
   "source": [
    "## Softmax\n",
    "\n",
    "See this [excellent blog post](https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/) from Eli Bendersky for derivation of softmax backprop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JwzdHlJbg5dg"
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "  def __init__(self, in_features, out_features, weight_scale = 1e-3):\n",
    "    # Initialise weight matrix of shape (in_features, out_features)\n",
    "    self.weights = np.random.randn(in_features, out_features) * weight_scale\n",
    "    self.bias = np.zeros(out_features)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - x: An ndarray containing input data, of shape (d_1, ..., d_k) such that \n",
    "      D = d_1 * ... * d_k is equal to `in_features`\n",
    "    Returns:\n",
    "    - out: output, of shape (1, out_features)\n",
    "    \"\"\"\n",
    "    self.x_before_flatten = x # cache\n",
    "    reshaped_x = x.flatten()[np.newaxis, :] # shape (1, D)\n",
    "    self.x_after_flatten = reshaped_x # cache\n",
    "\n",
    "    # @ is the matrix multiplication operator in Python\n",
    "    # reshaped_x's shape: (1 , D)\n",
    "    # self.weight's shape: (D , M) where M is out_features\n",
    "    # result of reshaped_x @ self.weights is of shape (1, M), so (1,10) for MNIST\n",
    "    out = reshaped_x @ self.weights + self.bias \n",
    "    return out\n",
    "  \n",
    "  def backward(self, dout):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "    - dout: An ndarray of the upstream derivatives, of shape (1, M) where M = out_features\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient w.r.t input x, of shape (d_1, ..., d_k)\n",
    "    - dw: Gradient w.r.t. weights w, of shape (D, M)\n",
    "    - db: Gradient w.r.t the biases, of shape (M, )\n",
    "    \"\"\"\n",
    "    dx = (dout @ self.weights.T).reshape(*self.x_before_flatten.shape)\n",
    "    dw = self.x_after_flatten.T @ dout # (D x 1) * (1 x M)\n",
    "    db = dout.squeeze() # turns (1, M) to (M, )\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PK_9KGVxOHl-"
   },
   "outputs": [],
   "source": [
    "def softmax_loss(scores, y):\n",
    "  \"\"\"\n",
    "  Inputs:\n",
    "  - scores: ndarray of scores, of shape (N, C) where C is the no. of classes\n",
    "  - y: ndarray of class labels, of shape (N,) where y[i] is the label for x[i] \n",
    "  Returns a tuple of:\n",
    "  - loss: Scalar value representing the softmax loss\n",
    "  - dscores: Gradient of loss w.r.t scores, of shape (N, C)\n",
    "  \"\"\"\n",
    "  N = scores.shape[0]\n",
    "\n",
    "  # for each sample, we shift the scores by the maximum value for numerical stability\n",
    "  # if you are confused, read the bolded section titled \"Practical issues: numerical stability\"\n",
    "  # from this page: https://cs231n.github.io/linear-classify/#softmax-classifier\n",
    "  shifted_scores = scores - np.amax(scores, axis=1, keepdims=True)\n",
    "\n",
    "  exp = np.exp(shifted_scores) # shape is (N, C)\n",
    "  probs = exp / exp.sum(axis=1, keepdims=True) # also (N, C)\n",
    "  cross_entropy = -1 * np.log( probs[np.arange(N), y] ) # shape is now (N,) -> scalar value per sample\n",
    "  loss = cross_entropy.mean() \n",
    "\n",
    "  # calculate derivatives. See https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/\n",
    "  dscores = probs.copy()\n",
    "  dscores[np.arange(N), y] -= 1\n",
    "  dscores /= N\n",
    "\n",
    "  return loss, dscores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZHBvj8_0YK8"
   },
   "source": [
    "### Putting it together\n",
    "\n",
    "Once we have the core components, we can put together the classes we've written so far to create a CNN model: \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFkgg5E_1EZg"
   },
   "outputs": [],
   "source": [
    "conv = Conv(1, 8, 3)  # 28x28x1 -> 26x26x8\n",
    "pool = MaxPool2x2() # 26x26x8 -> 13x13x8\n",
    "linear = Linear(13 * 13 * 8, 10)  # 13x13x8 -> 10\n",
    "\n",
    "def forward(image, y):\n",
    "  '''\n",
    "    Completes a forward pass of the CNN\n",
    "    - image is a numpy ndarray of shape (1 x 28 x 28) representing the greyscale image\n",
    "    - y is the corresponding digit\n",
    "  '''\n",
    "  # transform the image from [0, 255] to [0, 1]\n",
    "  out = conv.forward((image / 255))\n",
    "  out = pool.forward(out)\n",
    "  scores = linear.forward(out)\n",
    "\n",
    "  loss, dout = softmax_loss(scores, y)\n",
    "\n",
    "  return scores, loss, dout   \n",
    "\n",
    "def loss(image, label):\n",
    "  '''\n",
    "    Computes loss + completes a backward pass of the whole model\n",
    "  '''\n",
    "  grads = {}\n",
    "  # forward pass\n",
    "  scores, loss, dout = forward(image, label)\n",
    "\n",
    "  # Backprop\n",
    "  dpool, grads['W_linear'], grads['b_linear'] = linear.backward(dout)\n",
    "  dconv = pool.backward(dpool)\n",
    "  dx, grads['W_conv'], grads['b_conv'] = conv.backward(dconv)\n",
    "\n",
    "  return loss, grads\n",
    "\n",
    "\n",
    "# Challenge yourself: can you try to train the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ri_J-WT_z_Jy"
   },
   "source": [
    "## CNN in Keras\n",
    "\n",
    "So far we've walked you through how to implement a CNN from scratch in Python. Fortunately, we practically never have to write a CNN from scratch nowadays, thanks to high-level libraries like Keras that help abstract all the underlying complexity away from us. To cap off this notebook, let's use Keras to implement the **same** CNN that we've implemented above. We adapted this example from the [official Keras MNIST CNN example](https://keras.io/examples/vision/mnist_convnet/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1636922514322,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "WQRTH11M0Gun",
    "outputId": "e9d5f23d-96e0-4337-b3e9-a00004247112"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# split data\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# scale images\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = x_train[:,:,:, np.newaxis]\n",
    "x_test = x_test[:,:,:, np.newaxis]\n",
    "\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1636922514322,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "ishIxntsfLko",
    "outputId": "fb6dfe90-e4a7-44d9-c183-004978e58aac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train[0] before to_categorical:  5\n",
      "y_train[0] after to_categorical:  [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "y_train shape: (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "# one-hot encoding label vectors\n",
    "print(\"y_train[0] before to_categorical: \", y_train[0])\n",
    "\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "print(\"y_train[0] after to_categorical: \", y_train[0])\n",
    "\n",
    "print(\"y_train shape:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1636922514323,
     "user": {
      "displayName": "Yong Hoon Shin",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "00286622173541915419"
     },
     "user_tz": 0
    },
    "id": "5q7FtcaibIEx",
    "outputId": "c9cc33b9-3cd9-4403-e233-91863e63bcb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 8)         80        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 8)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1352)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                13530     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 13,610\n",
      "Trainable params: 13,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28,28,1)),\n",
    "        layers.Conv2D(filters=8, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(10, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnOU1eVpbIko",
    "outputId": "84278345-5892-4666-ebfa-f3f21c4235e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12276/54000 [=====>........................] - ETA: 2:23 - loss: 0.3075 - accuracy: 0.9076"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "epochs = 1\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrsqaVRIbOke"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6zRug-OEjcS1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNhDdmJHOWjXfN8UAIqPRMz",
   "collapsed_sections": [
    "H4NzaVImUkSm",
    "8bWtkocrb8tF",
    "_6YCcVkiWYhk",
    "fwSHWO7aXtT3",
    "I_o6mV_WdcHJ",
    "ri_J-WT_z_Jy"
   ],
   "name": "CNN_from_scratch (solution).ipynb",
   "provenance": [
    {
     "file_id": "1cweG3jT-rOy2_qpoWUeeNFLzi2mQTdAM",
     "timestamp": 1636655614454
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
