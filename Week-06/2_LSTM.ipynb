{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2370187",
   "metadata": {},
   "source": [
    "# Long Short-term Memory model (LSTM)\n",
    "\n",
    "As it has been mentioned in the previous notebook, the neurons of recurrent neural networks (RNNs) have an ability to retain cell memory that also influences the perception of new inputs. It all works fine, when we are dealing with problems with large separation between the predicted value and input, we face so-called ***vanishing gradient*** problem.\n",
    "\n",
    "#### Vanishing gradient\n",
    "\n",
    "![sigmoid](https://miro.medium.com/max/1250/1*6A3A_rt4YmumHusvTvVTxw.png)\n",
    "\n",
    "In simple terms, during the process of training a neural network, we are multiplying weights of each layers. However, if these weights (less than 1) are multiplied many times, we reach an extremely small values. Therefore, the weights of those earlier layers will not be changed signifficantly and our network will not be able to learn long-term dependencies.\n",
    "\n",
    "This brings brings us to **LSTM** that introduces changes RNN in larger series analysis.\n",
    "\n",
    "\n",
    "## LSTM Structure\n",
    "\n",
    "![LSTM](http://adventuresinmachinelearning.com/wp-content/uploads/2017/09/LSTM-diagram.png)\n",
    "\n",
    "The main difference between the LSTM and RNN structure is that LSTM has special kind of units (so-called, **LSTM cells**) instead of regular neural network layers. In the following sections, we will have a look at each structural unit of LSTM cell.\n",
    "\n",
    "\n",
    "### Input\n",
    "\n",
    "As it can be seen in the figure above, the input value is first concatenated to the output of the previous cell. This new input is then squashed between -1 and 1 using $tanh$ activation function:\n",
    "\n",
    "$g = tanh(x_tU^g + h_tV^g + b^g)$\n",
    "\n",
    "Here $U$, $V$ and $b$ are weights and bias respectively (*note that exponents g does not imply raising in power, but rather show that these are weights and biases at the input gate*).\n",
    "\n",
    "This squashed input is then multiplied element-wise by the input layer of sigmoid activated nodes. The existance of sigmoid function (output in the range of 0-1) allows to \"eliminate\" insignifficant . The output of the sigmoid nodes can be expressed as:\n",
    "\n",
    "$i = \\sigma(x_tU^i + h_tV^i + b^i)$\n",
    "\n",
    "After element-wise multiplication, we get the output of our input section:\n",
    "\n",
    "$g*i$\n",
    "\n",
    "### Forget gate\n",
    "\n",
    "The output of the input section is then passed through the forget gate loop. LSTM at this section has an internal state variable $s_t$ which is added to the lagged state ($s_{t-1}$). This addition operation is used instead of multiplication allows to reduce the risk of vanishing gradients while the forget gate itself helps model to learn only signifficant state variables.\n",
    "\n",
    "Mathematically, the forget gate output can be expressed as:\n",
    "\n",
    "$f = \\sigma(x_tU^f + h_tV^f + b^f)$\n",
    "\n",
    "Multiplying this by the previous state value and adding the input gate output, we get the new state value:\n",
    "\n",
    "$s_t = s_{t-1}*f+g*i$\n",
    "\n",
    "### Output gate\n",
    "\n",
    "At the output gate, we have to multiply the squashed ($tanh$ activation function) current state value by the forget output passed through the sigmoid function. In mathematical terms:\n",
    "\n",
    "$h_t = tanh(s_t)*\\sigma(x_tU^o + h_tV^o + b^o)$\n",
    "\n",
    "## Keras implementation\n",
    "\n",
    "As we have now covered all required theory, we are going to look how all of it can be implemented in Keras (*we are not going to be covering implementation from scratch*).\n",
    "\n",
    "Let's create a simple sine function dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfaf2359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b3b002",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.arange(0,100,0.5) \n",
    "y_train = np.sin(X_train)\n",
    "\n",
    "X_test = np.arange(100,200,0.5) \n",
    "y_test = np.sin(X_test)\n",
    "\n",
    "train_series = y_train.reshape((len(y_train), 1))\n",
    "test_series  = y_test.reshape((len(y_test), 1))\n",
    "\n",
    "plt.plot(X_train, y_train)\n",
    "plt.plot(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39e03d5f",
   "metadata": {},
   "source": [
    "On the other hand, as you might remember from the RNN notebook, the sequential data models are trained by passing array of lagged data as an input and future data as the output. Let's create function, that translates our dataset into such batches (for the start, we will use look-back period of 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3482c455",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset(data, look_back):\n",
    "    X, y = [], []\n",
    "    \n",
    "    for i in range(len(data) - look_back):\n",
    "        a = data[i:(i + look_back), ]\n",
    "        X.append(a)\n",
    "        y.append(data[i + look_back, ])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "look_back = 1\n",
    "X_train, y_train = dataset(train_series, look_back)\n",
    "X_test, y_test = dataset(test_series, look_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bf6a9e",
   "metadata": {},
   "source": [
    "After splitting data into batches, we can now create our simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3681f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    #Create model\n",
    "    ___\n",
    "])\n",
    "\n",
    "#Compile your model\n",
    "___\n",
    "\n",
    "#Fit model\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bfde92",
   "metadata": {},
   "source": [
    "Finally, let's test our model with the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9245f7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions  = model.predict(X_test).reshape(199, 1)\n",
    "plt.plot(test_predictions)\n",
    "plt.plot(y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
