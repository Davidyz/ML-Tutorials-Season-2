{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "442405e7",
   "metadata": {},
   "source": [
    "# Actor Critic Methods\n",
    "\n",
    "### Issues with REINFORCE\n",
    "\n",
    "In the previous notebook, we learned one of the policy gradient algorithms - REINFORCE. Even though it is a relatively easy and successful method, it has some drawbacks:\n",
    "\n",
    "- As the samples are random, there is a huge variability in probabilities and reward value that translates into fluctuating gradients.\n",
    "- REINFORCE fails to learn from trajectories that has a cumulative reward of 0 making it a slower algorithm.\n",
    "\n",
    "### Reducing variance\n",
    "\n",
    "One of the ways of reducing such variance is to subtract a baseline from the cumulative reward. Smaller rewards will translate into smaller gradients which will make training smoother.\n",
    "\n",
    "Let's look at the following example, let's say that $\\nabla_{\\theta}log\\pi_{\\theta}(\\tau) = [0.5, 0.2, 0.3]$ and $R(\\tau) = [1000, 1001, 1002]$.\n",
    "\n",
    "In such case the variance is **23286.8**.\n",
    "\n",
    "If we happen to decrease the rewards by constant value of 1001, the variance drops to **0.1633**.\n",
    "\n",
    "This becomes the basis of Actor Critic methods - from the mathematical perspective, it's the same REINFORCE algorithm (*kind of*) if we happen to reduce the cumulative reward function by a baseline function.\n",
    "\n",
    "### Actor Critic methods\n",
    "\n",
    "As it has already been mentioned, the actor critic methods functions according to the same function - the only difference is a baseline function that reduces the cumulative reward.\n",
    "\n",
    "From the structural standpoint, Actor Critic methods has two parts:\n",
    " - The Critic that estimates the value function (the new function we get by subtracting the accumulative reward)\n",
    " - The actor that updates policy\n",
    " \n",
    " The following picture demonstrates some of the existing Actor Critic methods. For this tutorial, however, we will analyze the Advantage Actor Critic algorithm (A2C for short). \n",
    "![actor-critic](https://miro.medium.com/max/2000/1*T1zTYVLkMNngE09fOqTkSA.png)\n",
    "\n",
    "### Advantage Actor Critic (A2C)\n",
    "\n",
    "As we can see from the figure above, the Advantage Actor-Critic algorithm includes the advantage function $A(s_t, a_t)$. In simple terms the advantage term tells us how much better it is to take an action $a_t$ instead of other actions at state $s_t$. Mathematically, this can be written as:\n",
    "$$\n",
    "A(s_t, a_t) = Q(s_t, a_t) - V(s_t)\n",
    "$$\n",
    "\n",
    "Here we are introduced to two new functions which might not make sense at the moment.\n",
    "\n",
    "- $Q(s_t, a_t)$ is called a **Q-value** and functions as a discounted future return. In other words, it shows how much return can be expected if we continue our trajectory until the end $T$.\n",
    "\n",
    "- V(s_t) shows how much return can be expected from the current state $s_t$ until the final step $T$.\n",
    "\n",
    "If both of these definitions sound similar to you, you are not wrong - the only difference lays in the first step. $Q$ value describes an expected reward **after** the action is taken, while $V$ shows reward of just being at the state.\n",
    "\n",
    "Consequently, the difference of these functions allows to see the advantage of certain action.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "Similar to the REINFORCE algorithm, we are not going to provide an implementation straight away - you can find it in the Challenge solution notebook.\n",
    "\n",
    "On the other hand, it is useful to discuss of the logic behind the A2C model.\n",
    "\n",
    "As it has been mentioned earlier, the A2C model consists of two parts:\n",
    "\n",
    "- Actor - takes a state and outputs the probability distribution of each action\n",
    "- Critic - takes a state and outputs expected reward for the range of actions\n",
    "\n",
    "Let's say we want to build a game agent in which there are only two possible actions at the given state. The following shared neural network could be used for both, actor and critic parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663fecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():    \n",
    "    \n",
    "    inputs = layers.Input(shape=(4,))\n",
    "    common = layers.Dense(128, activation=\"relu\")(inputs)\n",
    "    action = layers.Dense(2, activation=\"softmax\")(common)\n",
    "    critic = layers.Dense(1)(common)\n",
    "\n",
    "    model = keras.Model(inputs=inputs, outputs=[action, critic])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3000c453",
   "metadata": {},
   "source": [
    "Such model would take the state as an input and output the probability distribution of both of the actions as well as the expected reward for the range of actions.\n",
    "\n",
    "The overall pipeline of the code would look something like this:\n",
    "1. Extracting environment information (state, action, etc.)\n",
    "2. Passing state through the model to generate action and critic outputs\n",
    "3. Sample action from the probability distribution\n",
    "4. Calculating rewards\n",
    "5. Comparing rewards after taken trajectory to those calculated at the start of the trajectory to generate loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
