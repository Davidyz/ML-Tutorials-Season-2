{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb6d39bc",
   "metadata": {},
   "source": [
    "# 1. Q-Learning\n",
    "\n",
    "![q-learning algo](https://cdn-media-1.freecodecamp.org/images/k0IARc6DzE3NBl2ugpWkzwLkR9N4HRkpSpjw)\n",
    "\n",
    "Last week, we learned the basics of policy gradient algorithms through REINFORCE and A2C implementation. The working principle of such algorithms mostly aimed at optimizing the policy directly, so the action probability distribution ensures the maximum expected reward. This week, we are going to analyze ***value-based*** approach starting with Q-Learning algorithm\n",
    "\n",
    "In short, Q-Learning has the following characteristics:\n",
    "- **Model-free**. It estimates optimal policy without the need for any reward functions from the environment.\n",
    "- **Off-policy**. The function learns froms its actions and does not depend on the current policy.\n",
    "\n",
    "In constrast to the previous weeks, we are first going to analyze the mathematical derivation of Q-Learning before the explanation of the algorithm itself.\n",
    "\n",
    "## Mathematics\n",
    "\n",
    "To derive Q-Learning algorithm, let's take a step back and start from the basis of reinforcement learning as a whole.\n",
    "\n",
    "### Rewards\n",
    "\n",
    "As we have learned during the previous week, the reward is a feedback from the environment to the agent that helps to measure how *good* was the action. The total reward, therefore can be written as:\n",
    "$$\n",
    "R_t = r_{t+1}+r_{t+2}+...\n",
    "$$\n",
    "\n",
    "Even though such formula allows us to calculate the sum of the rewards, it would be quite unreasonable to use this as building function to our model, as it would potentially increase to infinity. One way of avoiding such problem is through the concept of **future reward** that, as we learned last tutorial, takes into account the decreasing importance of future rewards.\n",
    "$$\n",
    "R_t = r_{t+1}+\\gamma r_{t+2}+\\gamma^2 r_{t+2}+...\n",
    "$$\n",
    "\n",
    "Such formula can be simplified in the following way:\n",
    "$$\n",
    "R_t = r_{t+1}+\\gamma R_{t+1}\n",
    "$$\n",
    "\n",
    "\n",
    "### Policy\n",
    "\n",
    "Policy is a function that shows the probability of taking an action **a** at state **s**. In the policy gradient algorithms, we wanted to optimize the policy in a way that maximized the reward function.\n",
    "\n",
    "Important thing to note is that, since the policy is a probability distribution, the sum of all possible actions at a state must add-up to 1.\n",
    "\n",
    "$$\n",
    "\\sum_a \\pi (s, a) = 1\n",
    "$$\n",
    "\n",
    "### New notations\n",
    "\n",
    "We have already covered these concepts in the previous tutorial. On the other hand, the further formula derivation will require us to introduce some new notations for **immediate reward** and **transition probability**.\n",
    "\n",
    "**Imediate reward** can be imagined as an expected reward for going from state s to s' via action a.\n",
    "\n",
    "$$\n",
    "R_{ss'}^a = E[r_{t+1} | s_t = s, a_t = a, s_{t+1} = s']\n",
    "$$\n",
    "\n",
    "**Transition probability** can be defined as a probability of going from state s to s' via action a.\n",
    "\n",
    "$$\n",
    "P_{ss'}^a = E[s_{t+1} = s' | s_t = s, a_t = a]\n",
    "$$\n",
    "\n",
    "### Value functions\n",
    "\n",
    "In the advantage actor-critic tutorial, we already ran into **state value function** and **Q-value function**, that will play a more important role in Q-Learning models.\n",
    "\n",
    "**State value function** shows the expected total reward that can be received from the current state:\n",
    "\n",
    "$$\n",
    "V^{\\pi}(s) = E[R_t | s_t = s]\n",
    "$$\n",
    "\n",
    "**The Q-value function** shows the expected total reward that can be received after taking a specific action at current state:\n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s, a) = E[R_t | s_t = s, a_t = a]\n",
    "$$\n",
    "\n",
    "### Bellman equation\n",
    "\n",
    "Now, let's expand the Q-value function.\n",
    "$$\n",
    "Q^{\\pi}(s, a) = E[R_t | s_t = s, a_t = a]\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s, a) = E[r_{t+1} + \\gamma R_{t+1} | s_t = s, a_t = a]\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s, a) = E[r_{t+1} | s_t = s, a_t = a] + \\gamma E[R_{t+1} | s_t = s, a_t = a]\n",
    "$$\n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s, a) = \\sum_{s'} P_{ss'}^aR_{ss'}^a + \\gamma \\sum_{s'} P_{ss'}^aV^{\\pi}(s')\n",
    "$$\n",
    "\n",
    "If we assume that whenever the agent taks an action, it always ends up at the same next state, we can simplify the previous equation into the following one:\n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s, a) = R_{ss'}^a + \\gamma V^{\\pi}(s')\n",
    "$$\n",
    "\n",
    "Most commonly, this is written in the following manner.\n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s_t, a_t) = r(s_t, a_t) + \\gamma V^{\\pi}(s_{t+1})\n",
    "$$\n",
    "\n",
    "## Q-Learning\n",
    "\n",
    "![greedy_algo](https://miro.medium.com/max/375/0*rQ7hXKOPSxcR271w.gif)\n",
    "\n",
    "The Q-Learning is based on the **greedy policy** principle: agent always chooses the optimal next step (with the highest reward). In the mathematical context, we can write a relation between the state value and Q-value functions:\n",
    "\n",
    "$$\n",
    "V(s_t) = max_a Q(s_t, a)\n",
    "$$\n",
    "\n",
    "Plugging this into the previously derived equation, we get the Bellman equation for the Q-Learning.\n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s_t, a_t) = r(s_t, a_t) + \\gamma max_{a_{t+1}} Q(s_{t+1}, a_{t+1})\n",
    "$$\n",
    "\n",
    "In the descriptive terms, this equation tells that the of an action in a certain state is the **immediate reward** we get from taking the action and the **maximum expected reward** in the next state.\n",
    "\n",
    "\n",
    "## Implementation\n",
    "\n",
    "So far, we learned the main function that guides the process of Q-Learning; however, how does this translate to code?\n",
    "\n",
    "1. **Initiatizing Q-value space.** In order to record the data, model will have an associated Q-table containing all Q-values of the environment.\n",
    "2. **Extracting action probabilities**. At the start of each episode, we need to retrieve the probabilities of all actions in that state.\n",
    "3. **Taking action**. The action is taken according to the probability distribution.\n",
    "4. **Recording data**. We need to extract reward, next state after taking a sample action.\n",
    "5. **Taking next action**. The next action is taken according to the **greedy algorithm**.\n",
    "6. **Calculating loss**. In our case, the \"loss\" is the difference between the theoretical Q-value and the value we got after taking an action.\n",
    "7. **Updating Q-table**. After getting the loss, we can update our table by adding difference multiplied by the learning rate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
