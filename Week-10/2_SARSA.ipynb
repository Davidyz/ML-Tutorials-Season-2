{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "758f04bd",
   "metadata": {},
   "source": [
    "# 2. SARSA\n",
    "\n",
    "![sarsa](https://miro.medium.com/max/226/1*B4HaWwCxv4af2D6CEIGxyA.png)\n",
    "\n",
    "SARSA is an on-policy gradient where an action A taken in the current state S results a reward R. As result, the agent ends up in the next state S1, where it takes action A1 (this is where the SARSA name comes from). As an on-policy algorithm, SARSA updates its policy based on actions taken.\n",
    "\n",
    "## SARSA vs Q-Learning\n",
    "\n",
    "From the earlier explanation it may seem that SARSA is very similar to Q-Learning and it is partly right. The idea of storing Q-values in a tabular format remains the same; however, there is one signifficant difference - SARSA is on-policy algorithm, while Q-Learning is off-policy one.\n",
    "\n",
    "This will be explained more in-depth in the following mathematics section, but in short, SARSA algorithm uses the same current policy to choose A1 and update its Q-values, while Q-Learning uses greedy action to determine the next action.\n",
    "\n",
    "## Mathematics\n",
    "\n",
    "To better understand the difference between SARSA and Q-learning, let's look at SARSA's update function.\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha [r(s_t, a_t) + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]\n",
    "$$\n",
    "\n",
    "Now, let's look at the following equation we used to update Q-learning algorithm.\n",
    "$$\n",
    "Q(s_t, a_t) = Q(s_t, a_t) + \\alpha [r(s_t, a_t) + \\gamma max_{a_{t+1}} Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]\n",
    "$$\n",
    "\n",
    "As we can see, the only difference is that SARSA uses the **same behaviour policy** ($Q(s_{t+1}, a_{t+1})$) as a target Q value.\n",
    "\n",
    "Q-Learning, on the other hand, uses greedy policy to determine the target Q-value.\n",
    "\n",
    "## Implementation\n",
    "\n",
    "As SARSA is quite similar to Q-learning, it might be useful to implement SARSA (as it is not going to be used for the final challenge).\n",
    "\n",
    "#### 1. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1786cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "#Set up 'FrozenLake-v1' environment\n",
    "env = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c02046",
   "metadata": {},
   "source": [
    "#### 2. Initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb3b5d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining parameters\n",
    "epsilon = 0.9\n",
    "total_episodes = 10000\n",
    "max_steps = 100\n",
    "alpha = 0.85\n",
    "gamma = ___\n",
    " \n",
    "#Initializing Q-matrix\n",
    "Q = np.zeros((___, ___))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c88cdda",
   "metadata": {},
   "source": [
    "#### 3. Defining functions\n",
    "\n",
    "When it comes to functions, we will need function for **choosing an action** and **updating Q-table**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a19ee30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to choose the next action\n",
    "def choose_action(state):\n",
    "    action=0\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = np.argmax(Q[state, :])\n",
    "    return action\n",
    " \n",
    "#Function to learn the Q-value\n",
    "def update(state, state2, reward, action, action2):\n",
    "    predict = Q[state, action]\n",
    "    target = reward + gamma * Q[state2, action2]\n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b285467",
   "metadata": {},
   "source": [
    "#### 4. Training\n",
    "\n",
    "The training process itself involves taking an action from the action space, choosing the next action using our previously defined function and updating Q-value table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "394c0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the reward\n",
    "reward=0\n",
    " \n",
    "# Starting the SARSA learning\n",
    "for episode in range(total_episodes):\n",
    "    t = 0\n",
    "    state1 = env.reset()\n",
    "    action1 = choose_action(state1)\n",
    " \n",
    "    while t < max_steps:\n",
    "         \n",
    "        #Getting the next state after taking action1\n",
    "        state2, reward, done, info = ___\n",
    " \n",
    "        #Choosing the next action\n",
    "        action2 = choose_action(state2)\n",
    "         \n",
    "        #Learning the Q-value using our defined function\n",
    "        ___\n",
    " \n",
    "\n",
    "        state1 = state2\n",
    "        action1 = action2\n",
    "         \n",
    "        #Updating the respective vaLues\n",
    "        t += 1\n",
    "        reward += 1\n",
    "         \n",
    "        #If at the end of learning process\n",
    "        if done:\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
