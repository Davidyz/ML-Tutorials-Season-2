{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3c4a202",
   "metadata": {},
   "source": [
    "# 3. Deep Q-Network\n",
    "\n",
    "![dq](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/04/Screenshot-2019-04-16-at-5.46.01-PM.png)\n",
    "\n",
    "Regular Q-Learning trains by taking series of actions and creating a map that connects each state-action pair to its corresponding Q-value. In the case of complex, large environments, such approach becomes computationally intense and inefficient.\n",
    "\n",
    "One way of dealing with such problems is to use a **neural network** that would map input states to action, Q-value pairs. This is the basis of **deep Q-networks** (or **DQN** for short).\n",
    "\n",
    "\n",
    "## Algorithm\n",
    "\n",
    "The functioning of DQN can be divided into the following steps:\n",
    "\n",
    "1. Initializing main and target neural networks\n",
    "2. Choosing an action using the epsilon-greedy policy\n",
    "3. Updating weights using Bellman equation\n",
    "\n",
    "### Main and Target neural networks\n",
    "\n",
    "The main difference between the simple Q-Learning and DQN is the way of maping Q-values. DQN replaces the Q-table with a neural network and, rather than mapping a state-action pair to a Q-value, it maps input states to action, Q-value pairs.\n",
    "\n",
    "In addition, DQN uses **2 neural networks** with the same architecture, but different weights. The weigths of the target neural network are updated every N steps: this allows to keep Q-value stable, as the target model is used to calculate the prediction of future reward and is less susceptible to short fluctuations.\n",
    "\n",
    "![nn](https://miro.medium.com/max/875/0*9qs-EEw3iReB72Lf)\n",
    "\n",
    "From the structural perspective, each neural network receives a series of state inputs and outputs Q-values for each action.\n",
    "\n",
    "### Choosing an action\n",
    "\n",
    "As in Q-Learning, the action in DQN is chosen according to the same episilon-greedy policy.\n",
    "\n",
    "In DQN case, the neural network takes states as an input and produces Q-values for each action meaning that the action (output node) with the **largest Q-value** will be chosen.\n",
    "\n",
    "### Updating weights\n",
    "\n",
    "After the action is chosen, we need to update both networks according to our previously learned Bellman equation. In short, main neural network samples and trains on a batch of past episodes every few steps. The weights of main network is then coppied to the target network every N steps. This is called **experience replay**.\n",
    "\n",
    "**Experience replay** involves storing and replaying game states that RL algorithm is able to learn from. DQN uses it to learn on small batches to avoi skewing dsitrbution of the dataset. It is also not essential to train every step which allows us to control the network training speed.\n",
    "\n",
    "#### Bellman equation\n",
    "\n",
    "![bellmann](https://miro.medium.com/max/875/0*hVd8wqmFIEKQqGm9)\n",
    "\n",
    "Similar to Q-Learning, DQN will use Bellmann equation to update its weigths. According to the equation, the target value takes the following form.\n",
    "\n",
    "$$\n",
    "Q^{\\pi}(s_t, a_t) = r(s_t, a_t) + \\gamma max_{a_{t+1}} Q(s_{t+1}, a_{t+1})\n",
    "$$\n",
    "\n",
    "It is important to note that the **target network** is used to calculate this target. Such output is then passed to the **main network** as a target Q-value that is then being used to train the **main network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12e501e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
